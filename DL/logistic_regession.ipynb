{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac649d5",
   "metadata": {},
   "source": [
    "# 학습 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1299f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (10, 1) , t_data.shape =  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)   \n",
    "t_data = np.array([0, 0, 0, 0,  0,  0,  1,  1,  1,  1]).reshape(10,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5f38f",
   "metadata": {},
   "source": [
    "# 가중치W, 바이어스b 정의 (임의의 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f329549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.66237227]] , W.shape =  (1, 1) , b =  [0.71523983] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(1,1)   #0~1사이 난수값, 결과 값 내적 방지\n",
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7268b0",
   "metadata": {},
   "source": [
    "# Sigmoid(Z), 손실함수(cross-entropy) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7a8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):#output의 손실을 방지하기 위해\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def loss_func(x, t): \n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지, y=0 or 1인 경우 log내부 값 무한대\n",
    "    \n",
    "    z = np.dot(x,W) + b #행렬곱\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6f3fbd",
   "metadata": {},
   "source": [
    "## 수치미분 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a935cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x): #x 임의의 행렬\n",
    "    delta_x=1e-4\n",
    "    grad=np.zeros_like(x) #계산된 수치미분 값 저장 변수\n",
    "    it = np.nditer(x,flags=['multi_index'], op_flags=['readwrite'])\n",
    "    #모든 입력 변수에 대해 편미분 하기위한 iterator\n",
    "    #nider배열 반복해 요소 하나에 접근해 설정에 따라 값을 알려줌\n",
    "    \n",
    "    while not it.finished: #변수 개수 만큼\n",
    "        idx=it.multi_index #행렬에서 가리키는 값\n",
    "        \n",
    "        tmp=x[idx] #numpy 타입== mutable (변할 수 있음)이므로 원래 값 보관\n",
    "        x[idx]=float(tmp)+delta_x #미세하게 변화시킴\n",
    "        fx1=f(x) #f(x+delta_x)\n",
    "        \n",
    "        x[idx]=tmp-delta_x\n",
    "        fx2=f(x) #f(x-delta_x)\n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x)\n",
    "        \n",
    "        x[idx]=tmp\n",
    "        it.iternext()  #하나의 변수에 대한 수치미분 계산, 다음 변수로 넘어감\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee7eb2",
   "metadata": {},
   "source": [
    "## 학습, 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f9c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7    # log 무한대 발산 방지    \n",
    "    z = np.dot(x,W) + b # dot()nn *n 차원, 벡터*스칼라 사용X 권장 ->np.multiply\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d29daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  32.27899221774089 Initial W =  [[0.66237227]] \n",
      " , b =  [0.71523983]\n",
      "step =  0 error value =  15.075751405847486 W =  [[0.24693545]] , b =  [0.66169575]\n",
      "step =  400 error value =  3.1490261390372725 W =  [[0.43473305]] , b =  [-4.13167904]\n",
      "step =  800 error value =  1.7810327848178746 W =  [[0.45403878]] , b =  [-5.65007809]\n",
      "step =  1200 error value =  1.5163772972963316 W =  [[0.53121879]] , b =  [-6.67775596]\n",
      "step =  1600 error value =  1.3513854678483368 W =  [[0.59238585]] , b =  [-7.48999316]\n",
      "step =  2000 error value =  1.2351908269886909 W =  [[0.64384341]] , b =  [-8.17180651]\n",
      "step =  2400 error value =  1.1472012463762413 W =  [[0.68870413]] , b =  [-8.76515759]\n",
      "step =  2800 error value =  1.0772852845791887 W =  [[0.72875061]] , b =  [-9.29405663]\n",
      "step =  3200 error value =  1.0197970677098782 W =  [[0.76510662]] , b =  [-9.77362343]\n",
      "step =  3600 error value =  0.9713046164002582 W =  [[0.79853003]] , b =  [-10.21404348]\n",
      "step =  4000 error value =  0.9295831817968653 W =  [[0.82955823]] , b =  [-10.62253111]\n",
      "step =  4400 error value =  0.8931173295705842 W =  [[0.85858737]] , b =  [-11.00439829]\n",
      "step =  4800 error value =  0.8608333189639673 W =  [[0.88591856]] , b =  [-11.36367912]\n",
      "step =  5200 error value =  0.8319455663032527 W =  [[0.9117866]] , b =  [-11.70351549]\n",
      "step =  5600 error value =  0.8058637981485919 W =  [[0.93637837]] , b =  [-12.02640636]\n",
      "step =  6000 error value =  0.782134412416476 W =  [[0.95984534]] , b =  [-12.33437489]\n",
      "step =  6400 error value =  0.7604020684785018 W =  [[0.98231209]] , b =  [-12.62908408]\n",
      "step =  6800 error value =  0.7403837336630874 W =  [[1.00388248]] , b =  [-12.91191898]\n",
      "step =  7200 error value =  0.7218506701548115 W =  [[1.02464407]] , b =  [-13.18404655]\n",
      "step =  7600 error value =  0.7046156374629885 W =  [[1.04467142]] , b =  [-13.44645996]\n",
      "step =  8000 error value =  0.6885236115953401 W =  [[1.06402863]] , b =  [-13.70001218]\n",
      "step =  8400 error value =  0.6734449308563817 W =  [[1.08277122]] , b =  [-13.94544168]\n",
      "step =  8800 error value =  0.6592701508160422 W =  [[1.10094763]] , b =  [-14.18339247]\n",
      "step =  9200 error value =  0.6459061254347727 W =  [[1.11860041]] , b =  [-14.4144299]\n",
      "step =  9600 error value =  0.6332729824966913 W =  [[1.13576716]] , b =  [-14.63905322]\n",
      "step =  10000 error value =  0.621301761147535 W =  [[1.15248128]] , b =  [-14.85770573]\n",
      "step =  10400 error value =  0.6099325463400801 W =  [[1.16877259]] , b =  [-15.07078304]\n",
      "step =  10800 error value =  0.599112980870462 W =  [[1.18466783]] , b =  [-15.27863982]\n",
      "step =  11200 error value =  0.5887970676270206 W =  [[1.20019111]] , b =  [-15.48159538]\n",
      "step =  11600 error value =  0.578944197245227 W =  [[1.21536419]] , b =  [-15.67993835]\n",
      "step =  12000 error value =  0.5695183525377504 W =  [[1.23020686]] , b =  [-15.87393058]\n",
      "step =  12400 error value =  0.5604874528097686 W =  [[1.24473711]] , b =  [-16.06381044]\n",
      "step =  12800 error value =  0.5518228097942621 W =  [[1.25897141]] , b =  [-16.24979562]\n",
      "step =  13200 error value =  0.5434986733473234 W =  [[1.27292481]] , b =  [-16.4320855]\n",
      "step =  13600 error value =  0.5354918498490967 W =  [[1.28661117]] , b =  [-16.61086322]\n",
      "step =  14000 error value =  0.5277813798977574 W =  [[1.30004324]] , b =  [-16.78629745]\n",
      "step =  14400 error value =  0.5203482646660459 W =  [[1.3132328]] , b =  [-16.95854389]\n",
      "step =  14800 error value =  0.5131752324354774 W =  [[1.32619077]] , b =  [-17.12774666]\n",
      "step =  15200 error value =  0.5062465384900923 W =  [[1.33892726]] , b =  [-17.29403938]\n",
      "step =  15600 error value =  0.49954779285564177 W =  [[1.35145167]] , b =  [-17.45754626]\n",
      "step =  16000 error value =  0.4930658113988056 W =  [[1.36377276]] , b =  [-17.61838296]\n",
      "step =  16400 error value =  0.48678848661628127 W =  [[1.37589872]] , b =  [-17.7766574]\n",
      "step =  16800 error value =  0.4807046750954844 W =  [[1.38783718]] , b =  [-17.93247044]\n",
      "step =  17200 error value =  0.47480409915128635 W =  [[1.3995953]] , b =  [-18.0859165]\n",
      "step =  17600 error value =  0.46907726056570465 W =  [[1.4111798]] , b =  [-18.23708414]\n",
      "step =  18000 error value =  0.4635153647009 W =  [[1.42259698]] , b =  [-18.38605654]\n",
      "step =  18400 error value =  0.4581102535355331 W =  [[1.43385279]] , b =  [-18.53291192]\n",
      "step =  18800 error value =  0.4528543464039893 W =  [[1.4449528]] , b =  [-18.67772399]\n",
      "step =  19200 error value =  0.4477405874076508 W =  [[1.4559023]] , b =  [-18.82056224]\n",
      "step =  19600 error value =  0.44276239862341643 W =  [[1.46670627]] , b =  [-18.96149235]\n",
      "step =  20000 error value =  0.43791363836509706 W =  [[1.47736942]] , b =  [-19.10057637]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2  # 발산하는 경우(오버슈팅), 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(20001): # 우도함수에서 발생확률이 최대가 되도록 W,b 업데이트 \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400== 0): #계속 돌릴수록 손실함수값 작아짐, 수렴하지 않음-오버피팅\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc6593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.26152522e-07]] 0\n"
     ]
    }
   ],
   "source": [
    "(real_val, logical_val) = predict(3) \n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7829a8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99756338]] 1\n"
     ]
    }
   ],
   "source": [
    "(real_val, logical_val) = predict(17) \n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dca79",
   "metadata": {},
   "source": [
    "## 다중입력 (2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ade2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (9, 2) , t_data.shape =  (9, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[2,4],[4,11],[6,6],[8,5],[10,7],[12,16],[14,8],[16,3],[18,7]])  \n",
    "t_data = np.array([0, 0, 0, 0, 1,  1,  1,  1,  1]).reshape(9,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01ae02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.35727442]\n",
      " [0.52220294]] , W.shape =  (2, 1) , b =  [0.461313] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "#가중치,바이어스 정의\n",
    "W=np.random.rand(2,1)\n",
    "b=np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a1b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수 정의\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x,W) + b #행렬곱\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7289194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#수치미분 정의\n",
    "def numerical_derivative(f,x): #x 임의의 행렬\n",
    "    delta_x=1e-4\n",
    "    grad=np.zeros_like(x) #계산된 수치미분 값 저장 변수\n",
    "    it = np.nditer(x,flags=['multi_index'], op_flags=['readwrite'])\n",
    "    #모든 입력 변수에 대해 편미분 하기위한 iterator\n",
    "    #nider배열 반복해 요소 하나에 접근해 설정에 따라 값을 알려줌\n",
    "    \n",
    "    while not it.finished: #변수 개수 만큼\n",
    "        idx=it.multi_index #행렬에서 가리키는 값\n",
    "        \n",
    "        tmp=x[idx] #numpy 타입== mutable (변할 수 있음)이므로 원래 값 보관\n",
    "        x[idx]=float(tmp)+delta_x #미세하게 변화시킴\n",
    "        fx1=f(x) #f(x+delta_x)\n",
    "        \n",
    "        x[idx]=tmp-delta_x\n",
    "        fx2=f(x) #f(x-delta_x)\n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x)\n",
    "        \n",
    "        x[idx]=tmp\n",
    "        it.iternext()  #하나의 변수에 대한 수치미분 계산, 다음 변수로 넘어감\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8abcdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x,W) + b # dot()nn *n 차원, 벡터*스칼라 사용X 권장 ->np.multiply\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 x : numpy type\n",
    "def predict(x):\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8afa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  22.612571610205872 Initial W =  [[0.35727442]\n",
      " [0.52220294]] \n",
      " , b =  [0.461313]\n",
      "step =  0 error value =  12.066845420724032 W =  [[0.15857673]\n",
      " [0.26413231]] , b =  [0.42436905]\n",
      "step =  400 error value =  2.2500793929971645 W =  [[ 0.42324972]\n",
      " [-0.08509422]] , b =  [-2.66753582]\n",
      "step =  800 error value =  1.5821751671846302 W =  [[ 0.53835418]\n",
      " [-0.02547492]] , b =  [-4.29005138]\n",
      "step =  1200 error value =  1.2750713118972703 W =  [[0.62503358]\n",
      " [0.00962095]] , b =  [-5.39454352]\n",
      "step =  1600 error value =  1.095054365885111 W =  [[0.69508032]\n",
      " [0.03469101]] , b =  [-6.24105089]\n",
      "step =  2000 error value =  0.97445930819223 W =  [[0.75416202]\n",
      " [0.05468897]] , b =  [-6.93410939]\n",
      "step =  2400 error value =  0.8865878254402473 W =  [[0.80541277]\n",
      " [0.0718209 ]] , b =  [-7.52575009]\n",
      "step =  2800 error value =  0.8187810045967847 W =  [[0.85075013]\n",
      " [0.08723009]] , b =  [-8.04546555]\n",
      "step =  3200 error value =  0.764250750498313 W =  [[0.89143689]\n",
      " [0.10156861]] , b =  [-8.51151528]\n",
      "step =  3600 error value =  0.7190192568829062 W =  [[0.92835254]\n",
      " [0.11523218]] , b =  [-8.93595562]\n",
      "step =  4000 error value =  0.6805935019991685 W =  [[0.9621376]\n",
      " [0.1284702]] , b =  [-9.32714846]\n",
      "step =  4400 error value =  0.6473265389705074 W =  [[0.99327611]\n",
      " [0.14144227]] , b =  [-9.691123]\n",
      "step =  4800 error value =  0.6180833085817108 W =  [[1.02214554]\n",
      " [0.15424969]] , b =  [-10.03236521]\n",
      "step =  5200 error value =  0.5920539031359864 W =  [[1.04904833]\n",
      " [0.16695416]] , b =  [-10.35430018]\n",
      "step =  5600 error value =  0.5686434496005024 W =  [[1.07423242]\n",
      " [0.17958973]] , b =  [-10.65960012]\n",
      "step =  6000 error value =  0.5474041850740459 W =  [[1.09790498]\n",
      " [0.19217094]] , b =  [-10.95038832]\n",
      "step =  6400 error value =  0.5279919253675806 W =  [[1.12024175]\n",
      " [0.20469866]] , b =  [-11.22837863]\n",
      "step =  6800 error value =  0.5101372342075378 W =  [[1.14139352]\n",
      " [0.2171645 ]] , b =  [-11.49497341]\n",
      "step =  7200 error value =  0.4936257763282168 W =  [[1.1614907 ]\n",
      " [0.22955426]] , b =  [-11.75133412]\n",
      "step =  7600 error value =  0.47828459201497714 W =  [[1.18064666]\n",
      " [0.24185057]] , b =  [-11.99843318]\n",
      "step =  8000 error value =  0.46397229792619415 W =  [[1.19896022]\n",
      " [0.2540349 ]] , b =  [-12.23709284]\n",
      "step =  8400 error value =  0.4505719569922142 W =  [[1.21651762]\n",
      " [0.26608906]] , b =  [-12.46801474]\n",
      "step =  8800 error value =  0.43798580351514677 W =  [[1.23339408]\n",
      " [0.27799621]] , b =  [-12.69180272]\n",
      "step =  9200 error value =  0.42613128345537005 W =  [[1.24965524]\n",
      " [0.28974155]] , b =  [-12.90898066]\n",
      "step =  9600 error value =  0.4149380434478813 W =  [[1.26535833]\n",
      " [0.30131258]] , b =  [-13.12000651]\n",
      "step =  10000 error value =  0.4043456147303962 W =  [[1.28055319]\n",
      " [0.3126993 ]] , b =  [-13.32528344]\n",
      "step =  10400 error value =  0.3943016129211254 W =  [[1.29528331]\n",
      " [0.3238941 ]] , b =  [-13.52516884]\n",
      "step =  10800 error value =  0.38476032524405723 W =  [[1.30958662]\n",
      " [0.3348916 ]] , b =  [-13.71998149]\n",
      "step =  11200 error value =  0.37568159180346933 W =  [[1.32349625]\n",
      " [0.34568843]] , b =  [-13.91000746]\n",
      "step =  11600 error value =  0.36702991211864056 W =  [[1.33704123]\n",
      " [0.35628299]] , b =  [-14.09550486]\n",
      "step =  12000 error value =  0.3587737257052085 W =  [[1.35024704]\n",
      " [0.36667517]] , b =  [-14.27670783]\n",
      "step =  12400 error value =  0.3508848282091965 W =  [[1.36313614]\n",
      " [0.37686607]] , b =  [-14.45382971]\n",
      "step =  12800 error value =  0.34333789391602976 W =  [[1.37572838]\n",
      " [0.38685784]] , b =  [-14.62706581]\n",
      "step =  13200 error value =  0.3361100823474665 W =  [[1.3880414]\n",
      " [0.3966534]] , b =  [-14.79659563]\n",
      "step =  13600 error value =  0.3291807118012329 W =  [[1.40009092]\n",
      " [0.4062563 ]] , b =  [-14.96258474]\n",
      "step =  14000 error value =  0.322530986554676 W =  [[1.41189105]\n",
      " [0.41567057]] , b =  [-15.12518643]\n",
      "step =  14400 error value =  0.3161437673815735 W =  [[1.42345448]\n",
      " [0.42490057]] , b =  [-15.284543]\n",
      "step =  14800 error value =  0.31000337726199273 W =  [[1.43479267]\n",
      " [0.43395091]] , b =  [-15.44078697]\n",
      "step =  15200 error value =  0.30409543587585236 W =  [[1.44591606]\n",
      " [0.44282633]] , b =  [-15.59404209]\n",
      "step =  15600 error value =  0.2984067177895543 W =  [[1.45683417]\n",
      " [0.45153167]] , b =  [-15.74442413]\n",
      "step =  16000 error value =  0.2929250302680786 W =  [[1.46755571]\n",
      " [0.46007177]] , b =  [-15.89204172]\n",
      "step =  16400 error value =  0.28763910744242305 W =  [[1.47808872]\n",
      " [0.46845147]] , b =  [-16.03699694]\n",
      "step =  16800 error value =  0.28253851818809583 W =  [[1.4884406 ]\n",
      " [0.47667554]] , b =  [-16.17938591]\n",
      "step =  17200 error value =  0.27761358556315024 W =  [[1.49861825]\n",
      " [0.48474868]] , b =  [-16.31929929]\n",
      "step =  17600 error value =  0.2728553160458365 W =  [[1.50862804]\n",
      " [0.49267548]] , b =  [-16.45682276]\n",
      "step =  18000 error value =  0.2682553371233863 W =  [[1.51847596]\n",
      " [0.50046042]] , b =  [-16.59203737]\n",
      "step =  18400 error value =  0.26380584203323687 W =  [[1.52816759]\n",
      " [0.50810784]] , b =  [-16.72501992]\n",
      "step =  18800 error value =  0.2594995406598603 W =  [[1.53770818]\n",
      " [0.51562198]] , b =  [-16.85584328]\n",
      "step =  19200 error value =  0.2553296157531372 W =  [[1.54710267]\n",
      " [0.5230069 ]] , b =  [-16.98457665]\n",
      "step =  19600 error value =  0.251289683767088 W =  [[1.55635572]\n",
      " [0.53026656]] , b =  [-17.11128586]\n",
      "step =  20000 error value =  0.247373759727184 W =  [[1.56547172]\n",
      " [0.53740476]] , b =  [-17.23603356]\n"
     ]
    }
   ],
   "source": [
    "earning_rate = 1e-2  # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in  range(20001):   #모든 파라미터 동시에 업데이트(벡터화 구현이 더 이상적)\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400== 0): #계속 돌릴수록 손실함수값 작아짐\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b2e1f",
   "metadata": {},
   "source": [
    "# XOR 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60a60bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실함수, 수치미분함수 정의    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde89c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogicGate Class\n",
    "\n",
    "class LogicGate:\n",
    "    \n",
    "    def __init__(self, gate_name, xdata, tdata):  # xdata, tdata => numpy.array(...)\n",
    "        \n",
    "        self.name = gate_name\n",
    "        \n",
    "        # 입력 데이터, 정답 데이터 초기화\n",
    "        self.__xdata = xdata.reshape(4,2)\n",
    "        self.__tdata = tdata.reshape(4,1)\n",
    "        \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.__W = np.random.rand(2,1)  # weight, 2 X 1 matrix\n",
    "        self.__b = np.random.rand(1)\n",
    "                        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.__learning_rate = 1e-2\n",
    "        \n",
    "    # 손실함수\n",
    "    def __loss_func(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )      \n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def error_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "     # cross-entropy \n",
    "        return  -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1 - y)+delta ) )\n",
    "\n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self):\n",
    "        \n",
    "        f = lambda x : self.__loss_func()\n",
    "        \n",
    "        print(\"Initial error value = \", self.error_val())\n",
    "        \n",
    "        for step in  range(8001):\n",
    "            \n",
    "            self.__W -= self.__learning_rate * numerical_derivative(f, self.__W)\n",
    "    \n",
    "            self.__b -= self.__learning_rate * numerical_derivative(f, self.__b)\n",
    "    \n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val())\n",
    "                \n",
    "                \n",
    "    # 미래 값 예측 함수\n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        z = np.dot(input_data, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "    \n",
    "        if y > 0.5:\n",
    "            result = 1  # True\n",
    "        else:\n",
    "            result = 0  # False\n",
    "    \n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24a77f",
   "metadata": {},
   "source": [
    "## AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "171aa57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  5.356335015746674\n",
      "step =  0 error value =  5.286377470844243\n",
      "step =  400 error value =  1.509040088808328\n",
      "step =  800 error value =  1.1282890527249734\n",
      "step =  1200 error value =  0.9097813376496805\n",
      "step =  1600 error value =  0.7646953923416763\n",
      "step =  2000 error value =  0.6600581980109319\n",
      "step =  2400 error value =  0.5805472909130298\n",
      "step =  2800 error value =  0.5179086294968621\n",
      "step =  3200 error value =  0.4672277644124635\n",
      "step =  3600 error value =  0.4253637905647412\n",
      "step =  4000 error value =  0.3902014230519888\n",
      "step =  4400 error value =  0.3602576910916748\n",
      "step =  4800 error value =  0.33445925815178623\n",
      "step =  5200 error value =  0.31200882543490926\n",
      "step =  5600 error value =  0.29230117119751464\n",
      "step =  6000 error value =  0.2748683427739617\n",
      "step =  6400 error value =  0.259342730089926\n",
      "step =  6800 error value =  0.24543151227512697\n",
      "step =  7200 error value =  0.23289856374974854\n",
      "step =  7600 error value =  0.22155138382964296\n",
      "step =  8000 error value =  0.2112314879290429\n",
      "AND_GATE \n",
      "\n",
      "[0 0] = 0 \n",
      "\n",
      "[0 1] = 0 \n",
      "\n",
      "[1 0] = 0 \n",
      "\n",
      "[1 1] = 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,0,0,1]) #and 정답데이터\n",
    "\n",
    "AND_obj=LogicGate(\"AND_GATE\",xdata,tdata)\n",
    "\n",
    "AND_obj.train()\n",
    "# AND GATE prediction\n",
    "print(AND_obj.name,\"\\n\")\n",
    "\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (signoid_val,logical_val)=AND_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f9d5a",
   "metadata": {},
   "source": [
    "## OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9180da7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  1.8277152691656233\n",
      "step =  0 error value =  1.8195223006536128\n",
      "step =  400 error value =  1.035926582490207\n",
      "step =  800 error value =  0.7622660402385871\n",
      "step =  1200 error value =  0.5979771851631374\n",
      "step =  1600 error value =  0.4892906157388449\n",
      "step =  2000 error value =  0.4125053669868566\n",
      "step =  2400 error value =  0.355625831624252\n",
      "step =  2800 error value =  0.31194860655646944\n",
      "step =  3200 error value =  0.27744528146801856\n",
      "step =  3600 error value =  0.24955648348574766\n",
      "step =  4000 error value =  0.22658264741089848\n",
      "step =  4400 error value =  0.20735360158623664\n",
      "step =  4800 error value =  0.19103894291045553\n",
      "step =  5200 error value =  0.17703404810834109\n",
      "step =  5600 error value =  0.1648888371717412\n",
      "step =  6000 error value =  0.15426175385646376\n",
      "step =  6400 error value =  0.14488917150836353\n",
      "step =  6800 error value =  0.13656453675555572\n",
      "step =  7200 error value =  0.1291238327902657\n",
      "step =  7600 error value =  0.12243524533352472\n",
      "step =  8000 error value =  0.1163916850649114\n",
      "OR_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "OR_obj = LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "\n",
    "OR_obj.train() \n",
    "# OR Gate prediction\n",
    "print(OR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = OR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb09c82",
   "metadata": {},
   "source": [
    "## XOR 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b43ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  3.2294859222851127\n",
      "step =  0 error value =  3.2178934694146983\n",
      "step =  400 error value =  2.7791509022906937\n",
      "step =  800 error value =  2.7742033182404757\n",
      "step =  1200 error value =  2.7730125964552492\n",
      "step =  1600 error value =  2.7727037942396553\n",
      "step =  2000 error value =  2.772620155825206\n",
      "step =  2400 error value =  2.7725969759199187\n",
      "step =  2800 error value =  2.7725904771734395\n",
      "step =  3200 error value =  2.772588644868442\n",
      "step =  3600 error value =  2.7725881268468\n",
      "step =  4000 error value =  2.7725879802027293\n",
      "step =  4400 error value =  2.7725879386641417\n",
      "step =  4800 error value =  2.7725879268943694\n",
      "step =  5200 error value =  2.772587923558985\n",
      "step =  5600 error value =  2.7725879226137216\n",
      "step =  6000 error value =  2.7725879223458207\n",
      "step =  6400 error value =  2.7725879222698926\n",
      "step =  6800 error value =  2.772587922248373\n",
      "step =  7200 error value =  2.7725879222422734\n",
      "step =  7600 error value =  2.772587922240545\n",
      "step =  8000 error value =  2.7725879222400556\n"
     ]
    }
   ],
   "source": [
    "#XOR\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,1,1,0]) #xor 정답데이터\n",
    "\n",
    "XOR_obj=LogicGate(\"XOR_GATE\",xdata,tdata)\n",
    "\n",
    "XOR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4e59e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_GATE \n",
      "\n",
      "[0 0] = 0 \n",
      "\n",
      "[0 1] = 0 \n",
      "\n",
      "[1 0] = 0 \n",
      "\n",
      "[1 1] = 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR GATE prediction -> 틀림\n",
    "print(AND_obj.name,\"\\n\")\n",
    "\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (signoid_val,logical_val)=XOR_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e468e",
   "metadata": {},
   "source": [
    "## NAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c011d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  2.983874470255881\n",
      "step =  0 error value =  2.973882490101613\n",
      "step =  400 error value =  1.6249519584777996\n",
      "step =  800 error value =  1.1870253563616546\n",
      "step =  1200 error value =  0.9462210230593727\n",
      "step =  1600 error value =  0.7899322146570983\n",
      "step =  2000 error value =  0.6787300695103999\n",
      "step =  2400 error value =  0.5949801076724933\n",
      "step =  2800 error value =  0.5294197999961494\n",
      "step =  3200 error value =  0.47662931098363515\n",
      "step =  3600 error value =  0.4331878413030851\n",
      "step =  4000 error value =  0.39681316517305726\n",
      "step =  4400 error value =  0.3659169415645448\n",
      "step =  4800 error value =  0.339356303570435\n",
      "step =  5200 error value =  0.31628640026221666\n",
      "step =  5600 error value =  0.29606851663996486\n",
      "step =  6000 error value =  0.278210518643327\n",
      "step =  6400 error value =  0.26232696552811463\n",
      "step =  6800 error value =  0.2481116463079293\n",
      "step =  7200 error value =  0.23531821781762155\n",
      "step =  7600 error value =  0.22374627147607035\n",
      "step =  8000 error value =  0.213231124521587\n",
      "NAND_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NAND\n",
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([1, 1, 1, 0])\n",
    "\n",
    "NAND_obj = LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "\n",
    "NAND_obj.train()\n",
    "# NAND Gate prediction\n",
    "print(NAND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e546470",
   "metadata": {},
   "source": [
    "## XOR Multi-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d193032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]  =  0\n",
      "\n",
      "[0 1]  =  1\n",
      "\n",
      "[1 0]  =  1\n",
      "\n",
      "[1 1]  =  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR 을 NAND + OR => AND 조합으로 계산함\n",
    "input_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "s1 = []    # NAND 출력\n",
    "s2 = []    # OR 출력\n",
    "\n",
    "new_input_data = []  # AND 입력\n",
    "final_output = []    # AND 출력\n",
    "\n",
    "for index in range(len(input_data)):\n",
    "    \n",
    "    s1 = NAND_obj.predict(input_data[index])  # NAND 출력\n",
    "    s2 = OR_obj.predict(input_data[index])    # OR 출력\n",
    "    \n",
    "    new_input_data.append(s1[-1])    # AND 입력\n",
    "    new_input_data.append(s2[-1])    # AND 입력\n",
    "    \n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(np.array(new_input_data))\n",
    "    \n",
    "    final_output.append(logical_val)    # AND 출력, 즉 XOR 출력    \n",
    "    new_input_data = []    # AND 입력 초기화\n",
    "\n",
    "\n",
    "for index in range(len(input_data)):    \n",
    "    print(input_data[index], \" = \", final_output[index], end='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b7ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
