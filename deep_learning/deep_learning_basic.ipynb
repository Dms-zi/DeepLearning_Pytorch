{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ac82eb",
   "metadata": {},
   "source": [
    "# 딥러닝 강의 요약\n",
    "## 개요\n",
    "- 회귀(Regression): 연속적인 데이터를 학습하여 결과를 바탕으로 임의의 데이터 결과 예측\n",
    "- 분류(Classification): 데이터의 분포를 학습하여 결과를 바탕으로 임의의 데이터가 어느 분포에 속하는지\n",
    "- 신경망(Neural Network): 인간의 뇌 세포 상호작용 모방\n",
    "     - 이전 뉴런 출력을 입력으로 받아 + - 가중치를 통해 전체 합하여 특정 입계치(threshold)를 넘으면 다음 뉴런으로 전달하는 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d457f55",
   "metadata": {},
   "source": [
    "## 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545b34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c88bb",
   "metadata": {},
   "source": [
    "## 수치 미분\n",
    "### 미분\n",
    "- 입력 변수 x가 미세하게 변할 때, 함수 f가 얼마나 변할 수 있는지 근처함수 변화량\n",
    "- 함수는 입력 변수의 미세한 변화에 얼마나 민감하게 반응 하는지\n",
    "\n",
    "### 편미분\n",
    "- 입력 변수가 하나 이상인 다변수 함수에서, 미분하고자 하는 변수 하나를 제외하고 나머지 변수들을 상수 취급해, 해당 변수만 미분\n",
    "- 각 변수 변화에 따른 변화량\n",
    "\n",
    "### 연쇄법칙\n",
    "- 합성함수를 구성하는 각 함수의 미분의 곱으로 합성합수를 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb4f72",
   "metadata": {},
   "source": [
    "### 파이썬으로 수치미분 구현\n",
    "#### 수치미분 정의 및 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8e2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x): # f==함수, 외부에서 def, lambda정의\n",
    "    delta_x=1e-4 #lim에 해당되는 작은값 10의 -4승\n",
    "    return (f(x+delta_x)-f(x-delta_x))/(2*delta_x) #전방차분 f(x+dx)-f(x), 후방차분보다 중앙차분 f(x+dx)-f(x-dx)/2dx이 오차가 적음\n",
    "                                                    #너무 작은 값일때 오버플로우가 발생하지 않도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a58973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return x**2 #f(x)=x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66bf0af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000000000012662\n"
     ]
    }
   ],
   "source": [
    "result=numerical_derivative(my_func,3)  #f'(3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fc8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func2(x):\n",
    "    return 3*x*(np.exp(x)) #3xe^x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a936730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.50150507518049\n"
     ]
    }
   ],
   "source": [
    "result=numerical_derivative(my_func2,2)  #f'(2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1643dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative2(f,x): #x 임의의 행렬\n",
    "    delta_x=1e-4\n",
    "    grad=np.zeros_like(x) #계산된 수치미분 값 저장 변수\n",
    "    it = np.nditer(x,flags=['multi_index'], op_flags=['readwrite'])\n",
    "    #모든 입력 변수에 대해 편미분 하기위한 iterator\n",
    "    #nider배열 반복해 요소 하나에 접근해 설정에 따라 값을 알려줌\n",
    "    \n",
    "    while not it.finished: #변수 개수 만큼\n",
    "        idx=it.multi_index #행렬에서 가리키는 값\n",
    "        \n",
    "        tmp=x[idx] #numpy 타입== mutable (변할 수 있음)이므로 원래 값 보관\n",
    "        x[idx]=float(tmp)+delta_x #미세하게 변화시킴\n",
    "        fx1=f(x) #f(x+delta_x)\n",
    "        \n",
    "        x[idx]=tmp-delta_x\n",
    "        fx2=f(x) #f(x-delta_x)\n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x)\n",
    "        \n",
    "        x[idx]=tmp\n",
    "        it.iternext()  #하나의 변수에 대한 수치미분 계산, 다음 변수로 넘어감\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "750b5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(input_ob):\n",
    "    x=input_ob[0]\n",
    "    y=input_ob[1]\n",
    "    \n",
    "    return (2*x+3*x*y+np.power(y,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba3f2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.        , 15.00000001])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=np.array([1.0,2.0])\n",
    "numerical_derivative2(func,input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47c267",
   "metadata": {},
   "source": [
    "## 지도,비지도 학습\n",
    "### 학습 종류\n",
    "- 지도학습(supervised): 학습할 데이터의 입력과 입력에 대응되는 정답을 이용해 데이터의 특성과 분포를 학습하고 미래 결과 예측\n",
    "    - 회귀,분류\n",
    "- 비지도학습(unsupervised): 학습할 데이터에 정답이 없음, 입력값만 존재, 입력값의 특성과 분포만을 파악\n",
    "     - 군집화\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e6347",
   "metadata": {},
   "source": [
    "### 지도 학습\n",
    "- Training data == 입력과 정답을 포함\n",
    "- Test data == 미지의 데이터, 미래 값 예측\n",
    "- 트레이닝 데이터 학습(상관관계 파악) -> 테스트 데이터 예측 \n",
    "\n",
    "#### 회귀\n",
    "- 트레이닝 데이터를 이용해 연속적인 수치 값 예측\n",
    "\n",
    "#### 분류\n",
    "- 트레이닝 데이터를 이용해 주어진 입력값이 어떤 종류인지 구별 (범주형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca8c11",
   "metadata": {},
   "source": [
    "### 비지도 학습\n",
    "- 입력 값에 대한 패턴, 특성 등을 학습을 통해 발견\n",
    "\n",
    "#### 군집화\n",
    "- 분류와 차이점\n",
    "    - 분류는 정답에 가장 대응하는 임의의 직선을 찾아 기준으로 삼음\n",
    "    - 군집화는 정답이 없어, 특성을 찾아 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5022d3",
   "metadata": {},
   "source": [
    "## 선형 회귀(Linear Regression)\n",
    "### 오차, 손실 함수(loss function)\n",
    "- training data의 정답(t)와 입력(x)에 대한 계산 값 y의 차이를 모두 더해 수식으로 나타냄\n",
    "- 각 오차(잔차) 값의 부호가 다르므로(최소 오차값 판별 불가) 합이 아닌 제곱수 이용\n",
    "    - (t-y)^2 = (t-[Wx+b])^2\n",
    "    - 양수값 계산해 오차값을 크게 판별\n",
    "    \n",
    "- loss function = Σ{(t-y)^2 = (t-[Wx+b])^2}/n = E(W,b)\n",
    "       - W,b에 영향 받음(training data에서 주어짐)\n",
    "\n",
    "- 손실함수가 최소 값을 갖도록 (W,b)를 구하는 것이 선형회귀의 최종 목적\n",
    "    \n",
    "#### 학습 개념\n",
    "1. training data 분석\n",
    "     - 입력(x)에 대해 출력(y) 관계 파악\n",
    "2. 가중치(기울기 W), 바이어스 b(y 절편) 찾음\n",
    "    - y=Wx+b\n",
    "\n",
    "- 오차\n",
    "    - t- y= t-(Wx+b) , t-정답\n",
    "    - 오차가 클 경우, 임의로 설정한 가중치와 바이어스 값이 잘못됨\n",
    "    - 오차가 작을 경우, 미래 값 예측 정확도 상승 예상\n",
    "    - 오차가 가장 작은 가중치 W, 바이어스 b 찾아야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba33726",
   "metadata": {},
   "source": [
    "#### 경사하강법(gradient decent algorithm)\n",
    "- 손실함수가 최소값을 갖는 가중치 W와 바이어스 b를 찾는 방법\n",
    "- 손실 함수(포물선)에 대한 최소값\n",
    "1. 임의의 가중치 W 선택\n",
    "2. W 에서의 직선의 기울기를 나타내는 편미분값(E'(W))\n",
    "    - E'(W) 양수일 때, W 왼쪽으로 이동(감소)\n",
    "    - E'(W) 음수일 때, W 오른쪽으로 이동(증가)\n",
    "    - W=W-a(E'(W)) , a=학습율(옵티마이저), w값의 감소 또는 증가 되는 비율/ W<0,a>0\n",
    "3. 미분값이 작아지는 방향으로 W 감소(or 증가) 시켜나감\n",
    "4. 기울기가 더 이상 작아지지 않은 곳 찾음(E(W)의 최소값)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7158bb",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀(Logistic Regression) - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa8323",
   "metadata": {},
   "source": [
    "### 분류\n",
    " 1. 정의\n",
    "  - Training Data 특성과 관계 등을 파악한 후, 미지의 입력 데에터에 대해 결과가 어떤 종류의 값으로 분류될 수 있는지 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d2934",
   "metadata": {},
   "source": [
    "### 알고리즘\n",
    "1. 트레이닝 데이터 특성과 분포를 나타내는 최적의 직선을 찾고(Linear Regression)\n",
    "  \n",
    "2. 그 직선을 기준으로 위(1) 또는 아래(0)로 분류 해주는 알고리즘 -> 정확도 높음\n",
    "\n",
    "(x,t)->Regression->Classification->True or False\n",
    "           (Wx+b)-> (sigmoid)\n",
    "             \n",
    "             \n",
    "3. 함수 값이 0과 1사이에 머물러있는 Sigmoid 함수 사용\n",
    " - 분류 시스템은 출력 값 y가 0 or 1만 가져야 하므로 적합\n",
    " - Wx+b=z에서 z가 어떤 값을 갖더라도 출력함수로 출력함수로 Sigmoid사용해 0.5보다 크면 결과로 1이 나올성이 높고, 작으면 0이 나올성 높음\n",
    " -> Sigmoid 함수의 실제 계산값 Sigmoid(z)는 결과가 나타날 확률 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0758b2",
   "metadata": {},
   "source": [
    "### 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef26ed",
   "metadata": {},
   "source": [
    "$y=\\frac{1}{1+e^{-(Wx+b)}} , t_{i}=0  or  1$\n",
    "  \n",
    "  일때,\n",
    "\n",
    "손실함수\n",
    "$E(W,b)=-\\sum_{i=1}^{n} {t_{i}logy_{i}+(1-t_{i})log(1-y_{i})}$\n",
    "  \n",
    "  \n",
    "- 수치미분으로 나타냄  \n",
    " $W=W-\\alpha \\frac{\\delta E(W,b)}{\\delta W}$\n",
    "   \n",
    " $b=b-\\alpha \\frac{\\delta E(W,b)}{\\delta b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0695e2",
   "metadata": {},
   "source": [
    "### 구현\n",
    "1. 학습을 위한 트레이닝 데이터 준비\n",
    " - python slicing or list comprehension등을 이용해 입력 x와 레이블 t를 numpy타입으로 분리(t=0 or 1)\n",
    "  \n",
    "  \n",
    "2. sigmoid 함수에 입력으로 들어가는 임의의 직선z=Wx+b (numpy random으로 W,b)\n",
    "  \n",
    "3. 손실함수 구현 \n",
    " - def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    " - def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # 아주작은값 델타, y=0 or 1 일때 log 내 값이 무한대\n",
    "    \n",
    "    z = np.dot(x,W) + b #행렬곱 dot\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "  - return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) )  #손실함수 엔트로피\n",
    "4. 학습률은 10 ^-3 ~ 10^-5 등\n",
    "  \n",
    "5. W,b구하기\n",
    "  - f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "  - W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52132b",
   "metadata": {},
   "source": [
    "## XOR 문제\n",
    "### 논리게이트\n",
    "1. 논리테이블(AND,OR,NAND,XOR)은 입력데이터 (x1,x2),정답데이터(t=0 or 1)인 머신러닝 트레이닝 데이터와 개념적으로 동일함\n",
    "\n",
    "2. 손실함수 이용해 Logistic Regression 알고리즘으로 데이터 분류하고 결과 예측 가능\n",
    "\n",
    "3. 논리게이트 class 정의-> Logistic Regression 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505eeda",
   "metadata": {},
   "source": [
    "## 로지스틱 클래스\n",
    "```class LogicGate:\n",
    "    def __init__(self,gate_name,xdata,tdata) #__xdata,__tdata,__W,__b초기화\n",
    "    def __lost_func(self) #손실함수 cross-entropy\n",
    "    def error_val(self) # 손실함수 값 계산\n",
    "    def train(self) #수치미분을 이용해 손실함수 최소값 찾는 method\n",
    "    def predict(self,xdata) #미래 값 예측 method\n",
    "   ``` \n",
    "## Usage\n",
    "```xdata=np.array(0,0],0,1],1,0],1,1]]) #입력데이터 생성\n",
    "tdata=np.array(0,0,0,1]) #정답데이터 생성(AND)\n",
    "\n",
    "AND_obj=LogicGate(\"AND_GATE\",xdata,tdata) #LogicGate 객체생성\n",
    "\n",
    "AND_obj.train() #손실함수 최소값 갖도록 학습\n",
    "\n",
    "AND_obj.predict() #임의의 데이터에 대해 결과 예측\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0621eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
